{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/austinAbraham/hello-world/blob/master/pyspark_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih1tLstkkRlv"
      },
      "source": [
        "# Mango DE PySpark Test\n",
        "\n",
        "This exercise is designed to test a candidates ability to translate requirements into a very small ETL script using PySpark.\n",
        "\n",
        "The first set of cells in this notebook install java and spark in the environment, a repo with a data generating function, and start a spark session. One common issue with using these setup instructions is slow response from the mirror used to download spark, so an alternative mirror is provided (commented out in code cell). However, you may need to shop around to find a mirror that is more appropriate for your location. Candidates are expected to be able to debug installation/setup issues themselves.\n",
        "\n",
        "Instructions for the test can be found below the setup cells.\n",
        "\n",
        "If you are new to Google colab, note that there is a directory browser button on the left. As you run through the setup cells and generate data for the test, files will be added here. The home directory for this workspace is /content and you can mount this to a personal Drive to persist files that are added here if you wish. For more information on colab, see the tutorial here: https://colab.research.google.com/notebooks/welcome.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PufeRRtJE3KI"
      },
      "source": [
        "# Install Java and Spark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -qN https://apache.mirrors.nublue.co.uk/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "#!wget -qN https://mirrors.gethosted.online/apache/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "\n",
        "!tar xf spark-3.0.3-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztmhPoaME9hf"
      },
      "source": [
        "# If having issues then uncomment the below line and run. Then re-run the previous two cells\n",
        "# !rm -rf spark-2.4.7-bin-hadoop2.7*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLP91rQkhTWd"
      },
      "source": [
        "!git clone https://github.com/MangoTheCat/de_tech_test_pyspark.git functions > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYwE9rxqFAlc"
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma7AV5WqFC_z"
      },
      "source": [
        "# Configure Java / Spark environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbBM0fyPFE84"
      },
      "source": [
        "# start a spark session to check that this is all working\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erjcUagzl14c"
      },
      "source": [
        "## Scenario overview\n",
        "\n",
        "An organisation recieves files, usually daily but with some gaps, which are saved to ~/data/main. The task is to do some basic cleaning and transformation of these files and to write them to a separate dataset. Imagine that the output you produce here will be registered as an external table in a Hive database, which an analysis team will query using a SQL tool.\n",
        "\n",
        "Files are generated in the first code cell below. Running this cell will produce a new file with random data for today's date. It will throw an error if a file already exists for today's date. Each file will be relatively small due to the restrictions of this environment, but treat these files as though they are large.\n",
        "\n",
        "Detailed instructions of the required transformations are expressed below. Feel free to use cells in this notebook while developing your solution, but you should develop a solution that can sit in a separate .py file. You should also include unit tests as appropriate. Remember that, if you have not mounted the drive of your colab runtime, any external files you create here will not persist. Please add a text cell describing any assumptions you have made about how your script should be run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYC95Ur8FQIn"
      },
      "source": [
        "from functions.generate_data import generate_data\n",
        "generate_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDmtn8VvRMWz"
      },
      "source": [
        "\n",
        "## Requirements\n",
        "\n",
        "1. Some values in field1 are missing. We know the value columns for these records are faulty so we should remove these rows entirely.\n",
        "\n",
        "2. Field2 also has some missing data, but we can work out what those values should be. If field3 is between 8 and 12, these missing values should be equal to 1. Otherwise, they should be 2.\n",
        "\n",
        "3. We need to add an extra dimension based on fields 1 and 2. A lookup csv has been provided in the data directory. You should add the lookup_val field to the dataset as a column named 'new_dimension'.\n",
        "\n",
        "4. If either field1 or field2 have a value of 10, the new_dimension column should have a value of 'XX'.\n",
        "\n",
        "5. The analysis team will often run queries over a specific year or a specific year/month. Take this into consideration when designing the output table structure.\n",
        "\n",
        "6. For analysis purposes, the team won't need field3. Please remove this field and aggregate on the remaining fields. All the value columns should be summed.\n",
        "\n",
        "7. A distinct count of the values in field3 should be included in the output.\n",
        "\n",
        "8. The analysts would also like a column added showing the proportion of val3 for each value of field4 over the total of val3 for the remaining dimension columns.\n",
        "\n",
        "9. It should be rare, but we may sometimes receive a replacement file for an old period (which will overwrite the old file in the data/main directory). We need to be able to use the transformation code on an adhoc basis to update that data in our output. The cluster we are working on is very busy, so we don't want this to take longer than it has to.\n",
        "\n",
        "10. Write your output to the home directory in a format and structure that you deem suitable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujVbvQlhG3r5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4a9e155-ab06-472e-c278-cec1468557c4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql.functions import expr, when, col, collect_list, lit\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.types import DecimalType\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "read_data(data_date)\n",
        "\n",
        "parameters : \n",
        "  - data_date, date string expected in yyyy-mm-dd format\n",
        "\n",
        "If data_date is not null the current date will be used to read the etxt file,\n",
        "otherwise the data file from the specified date will be read.\n",
        "\n",
        "The data frame will be returned\n",
        "\n",
        "\"\"\"\n",
        "def read_data(data_date):\n",
        "  # set data_date\n",
        "  date=data_date\n",
        "  if data_date == \"\":\n",
        "    date = datetime.today().strftime('%Y-%m-%d')\n",
        "    print(date)\n",
        "  #  reading data\n",
        "  data = spark.read.csv('./data/main/' + date + '/test_data.csv', header=True)\n",
        "\n",
        "  # returning dataframe\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "processing(df)\n",
        "\n",
        "parameters : \n",
        "  - df, data frame to apply transfirmations\n",
        "\n",
        "Dataframe passed in processed with multiple tranformation performed,\n",
        "  - row with a. null in field1 column is removed\n",
        "  - field2 column value set as 1 if field3 value between 8 and 12, inclusive \n",
        "    of the 8 and 12. Other wise set as 2.\n",
        "  - columns field1, field2, field3, field4 cast as integer type\n",
        "  - columns val1,val2,val3 cast as double type   \n",
        "\n",
        " The function returns the processed dataframe \n",
        "\n",
        "\"\"\"\n",
        "def processing(df):\n",
        "  # drop rows with nulls in field1\n",
        "  # set value for null in field2\n",
        "  df1 = df.dropna(how='any', subset='field1')\\\n",
        "    .withColumn('field2', when(col('field2')== 'null', \\\n",
        "                                  when(((col('field3') < 12) & (col('field3') > 8)), 1)\\\n",
        "                               .otherwise(2)).otherwise(col('field2')))\n",
        " \n",
        "  # cast field1, field2, field3, field4 to int\n",
        "  # cast val1, val2, val3 to double\n",
        "  df2 = df1\\\n",
        "    .withColumn('field1', col(\"field1\").cast(\"int\"))\\\n",
        "    .withColumn('field2', col(\"field2\").cast(\"int\"))\\\n",
        "    .withColumn('field3', col(\"field3\").cast(\"int\"))\\\n",
        "    .withColumn('field4', col(\"field4\").cast(\"int\"))\\\n",
        "    .withColumn('val1', col(\"val1\").cast('double'))\\\n",
        "    .withColumn('val2', col(\"val2\").cast('double'))\\\n",
        "    .withColumn('val3', col(\"val3\").cast('double'))  \n",
        "\n",
        "  # return df\n",
        "  return df2\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "data_joining(df, df_l)\n",
        "\n",
        "parameters : \n",
        "  - df, main data frame to join\n",
        "  - df_l, lookup data frame\n",
        "\n",
        "  - Dataframes df and df_l are joined uner conditions field1 == f1 and field2 == f2\n",
        "  - Column lookup_val is renamed 'new_dimension' \n",
        "\n",
        " The function returns the joined dataframe \n",
        "\n",
        "\"\"\"\n",
        "def data_joining(df, df_l):\n",
        "\n",
        "  #  renaming field1 in look-up to f1 for ease\n",
        "  df_l = df_l.withColumnRenamed('field1', 'f1')\n",
        "\n",
        "  # joining data frames\n",
        "  # renaming lookup_val to new dimension\n",
        "  # modiflyin values in new_dimension based on condition\n",
        "  #  dropping columns f1, f2\n",
        "  data_joined = df.join(df_l, on=[col('field1')==col('f1'), col('field2')==col('f2')])\\\n",
        "    .withColumnRenamed('lookup_val', 'new_dimension')\\\n",
        "    .withColumn('new_dimension', when(((col('field1') == 10) | (col('field2') == 10)), 'XX')\\\n",
        "                .otherwise(col('new_dimension')))\\\n",
        "    .drop('f1', 'f2')            \n",
        "\n",
        "  # returning dataframe\n",
        "  return data_joined    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "aggre(df, df_l)\n",
        "\n",
        "parameters : \n",
        "  - df, main data frame to aggregate\n",
        "\n",
        "  - all value columns are summed\n",
        "  - Relevance for val3 in each group in field 4 is calculated\n",
        "\n",
        " The function returns the joined dataframe \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def aggre(df):\n",
        "\n",
        "  # summing val1, val2, val3 to val_sum\n",
        "  # calcualtion of relevance of val3 with field4 as val3_relevance\n",
        "  # casting val3_relevance as decimal type\n",
        "  df1 = df\\\n",
        "    .withColumn('val_sum', expr('val1 + val2 + val3'))\\\n",
        "    .withColumn('val3_relevance', expr(\"val3/(SUM(val3) over (PARTITION BY field4)) \"))\\\n",
        "    .withColumn('val3_relevance', col('val3_relevance').cast(DecimalType(11,10)))\n",
        "\n",
        "\n",
        "  return df1\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  \"\"\"  \n",
        "    # casting system arguments into a string arragy\n",
        "    args = str(sys.argv)\n",
        "\n",
        "    args_length = len(args)\n",
        "\n",
        "    if args_length > 3:\n",
        "      date_arg = args[3]\n",
        "    else:\n",
        "      # assigned empty value to use the day's date for processing\n",
        "      date_arg = None\n",
        "\n",
        "    # reading look_up and output file locations\n",
        "    lookp_up_arg = args[2]      \n",
        "    output_arg = args[1]\n",
        "      \n",
        "  \"\"\"\n",
        "  # reading file date to process \n",
        "  date_arg = input(\"Enter date or leave empty: \")\n",
        "\n",
        "  #  reading output folder path\n",
        "  output_arg = './data/output'\n",
        "\n",
        "  # reading look-up data\n",
        "  data_lookup = spark.read.csv('./data/lookup.csv', header=True)\n",
        "\n",
        "  # reading data file\n",
        "  data = read_data(date_arg)\n",
        "\n",
        "  # processing data file\n",
        "  data_work_copy = processing(data)\n",
        "\n",
        "  # joining main data and look-up\n",
        "  data_joined = data_joining(data_work_copy, data_lookup)\n",
        "\n",
        "  # data aggregations\n",
        "  data_agg = aggre(data_joined)\n",
        "\n",
        "  # setting date output write \n",
        "  date = date_arg\n",
        "  if date_arg == \"\":\n",
        "    date = datetime.today().strftime('%Y-%m-%d')\n",
        "\n",
        "\n",
        "\n",
        "  # adding date column, processed_date column\n",
        "  data_agg = data_agg.withColumn('date', lit(date))\\\n",
        "  .withColumn('date_processed', lit(datetime.today().strftime('%Y-%m-%d')))\\\n",
        "  .drop('field3')\n",
        "\n",
        "  # checking if output file exists \n",
        "  if Path(output_arg).exists():\n",
        "    #  reading output file\n",
        "    data_store = spark.read.csv(output_arg, header=True)\n",
        "\n",
        "    # creating final dataframe by joining current and main\n",
        "    data_final = data_store.union(data_agg)\n",
        "\n",
        "    # writing data to temp location\n",
        "    data_final.write.csv(path='./data/temp_csv', mode='overwrite', header=True)\n",
        "\n",
        "    # reading data from temp and overwriting output file\n",
        "    df_final = spark.read.csv(path='./data/temp', header=True)\n",
        "    df_final.write.csv(path=output_arg, mode='overwrite', header=True)\n",
        "\n",
        "\n",
        "    # data_final.show(10)\n",
        "  else:\n",
        "      data_agg.write.csv(path=output_arg, mode='overwrite', header=True)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        " \n",
        "\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter date or leave empty: \n",
            "2021-07-26\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}